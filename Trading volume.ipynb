{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model import FeatureDependentMarkovChain\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(\"IBM\")\n",
    "df[\"RET\"] = df[\"Adj Close\"].pct_change()\n",
    "df[\"RET1D\"] = df.RET.shift(1).rolling(1).mean()\n",
    "df[\"RET5D\"] = df.RET.shift(1).rolling(5).mean()\n",
    "df[\"RET20D\"] = df.RET.shift(1).rolling(20).mean()\n",
    "df[\"RET60D\"] = df.RET.shift(1).rolling(60).mean()\n",
    "vix = yf.download(\"^VIX\")[\"Adj Close\"].shift(1)\n",
    "df[\"VIX\"] = vix[df.index]\n",
    "# df[\"Volume\"] = np.log(df.Volume)\n",
    "df[\"Volume60D\"] = df.Volume.shift(1).rolling(60).mean()\n",
    "df = df[df.index >= \"2000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "\n",
    "qt = QuantileTransformer()\n",
    "\n",
    "train_size = 3000\n",
    "val_size = 1000\n",
    "\n",
    "volume = df.Volume.values[:, None]\n",
    "volume_train = volume[:train_size]\n",
    "volume_val = volume[train_size:train_size+val_size]\n",
    "volume_test = volume[train_size+val_size:]\n",
    "volume_train = qt.fit_transform(volume_train).flatten()\n",
    "volume_val = qt.transform(volume_val).flatten()\n",
    "volume_test = qt.transform(volume_test).flatten()\n",
    "s_train = list(np.digitize(volume_train, np.linspace(0, 1+1e-8, n+1)) - 1)\n",
    "s_val = list(np.digitize(volume_val, np.linspace(0, 1+1e-8, n+1)) - 1)\n",
    "s_test = list(np.digitize(volume_test, np.linspace(0, 1+1e-8, n+1)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Volume.plot()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"Volume\")\n",
    "for x in qt.inverse_transform(np.linspace(0, 1, 6)[:,None]):\n",
    "    plt.axhline(x, ls='--', c='black')\n",
    "plt.savefig(\"figs/volume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[[\"VIX\", \"RET60D\"]].values\n",
    "features = np.hstack([features])\n",
    "scaler = QuantileTransformer(n_quantiles=1000, output_distribution='uniform')\n",
    "features_train = features[:train_size]\n",
    "features_val = features[train_size:train_size+val_size]\n",
    "features_test = features[train_size+val_size:]\n",
    "features_train = (scaler.fit_transform(features_train) - .5) * 2\n",
    "features_val = (scaler.transform(features_val) - .5) * 2\n",
    "features_test = (scaler.transform(features_test) - .5) * 2\n",
    "plt.plot(df.index, np.concatenate([features_train[:,0], features_val[:,0], features_test[:,0]]), label='VIX')\n",
    "plt.plot(df.index, np.concatenate([features_train[:,1], features_val[:,1], features_test[:,1]]), label='RET60D')\n",
    "plt.ylabel(\"features\")\n",
    "plt.savefig(\"figs/features.pdf\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8, suppress=True)\n",
    "P = np.zeros((n, n))\n",
    "for i in range(len(s_train) - 1):\n",
    "    P[s_train[i], s_train[i+1]] += 1\n",
    "P /= P.sum(axis=1)[:,None]\n",
    "np.round(P, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s_train), len(s_val), len(s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "s_train_missing = deepcopy(s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[train_size:train_size+val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "for idx in np.random.choice(np.arange(1, len(s_train) - 1), 1500, replace=False):\n",
    "    s_train_missing[idx] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "len_nan = []\n",
    "while i < len(s_train) - 1:\n",
    "    if ~np.isnan(s_train_missing[i]) and ~np.isnan(s_train_missing[i+1]):\n",
    "        i += 1\n",
    "        len_nan.append(0)\n",
    "    else:\n",
    "        j = 1\n",
    "        while np.isnan(s_train_missing[i+j]):\n",
    "            j += 1\n",
    "        i += j\n",
    "        len_nan.append(j - 1) \n",
    "plt.hist(len_nan, bins=80);\n",
    "plt.xlabel(\"consecutive missing entries\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.savefig(\"figs/consecutive.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([0, .2, .4, .6, .8, 1.])\n",
    "\n",
    "def evaluate(zero_features=False, verbose=False, missing=True, **kwargs):\n",
    "    model = FeatureDependentMarkovChain(n, **kwargs)\n",
    "    if missing:\n",
    "        s = s_train_missing\n",
    "    else:\n",
    "        s = s_train\n",
    "    if zero_features:\n",
    "        model.fit(s, features_train * 0, [len(s_train)], verbose=verbose)\n",
    "        train_ll = model.score(s, features_train * 0, [len(s_train)], average=True)\n",
    "        val_ll = model.score(s_val, features_val * 0, [len(s_val)], average=True)\n",
    "        test_ll = model.score(s_test, features_test * 0, [len(s_test)], average=True)\n",
    "        pred = model.predict(features_test * 0)[np.arange(len(s_test) - 1),s_test[:-1],:]\n",
    "    else:\n",
    "        model.fit(s, features_train, [len(s_train)], verbose=verbose)\n",
    "        train_ll = model.score(s, features_train, [len(s_train)], average=True)\n",
    "        val_ll = model.score(s_val, features_val, [len(s_val)], average=True)\n",
    "        test_ll = model.score(s_test, features_test, [len(s_test)], average=True)\n",
    "        pred = model.predict(features_test)[np.arange(len(s_test) - 1),s_test[:-1],:]\n",
    "    pred_median = []\n",
    "    for i in range(len(pred)):\n",
    "        pred_median.append(np.interp(.5, np.cumsum(np.append(0, pred[i])), z))\n",
    "    pred_score = np.mean(np.abs(volume_test[1:] - np.array(pred_median)))\n",
    "    return model, train_ll, val_ll, test_ll, pred_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting without missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_lap_states = nx.adjacency_matrix(nx.path_graph(5))\n",
    "\n",
    "model1, train1, val1, test1, pred1 = evaluate(zero_features=True, verbose=False, missing=False, lam_frob=0, n_iter=1)\n",
    "model2, train2, val2, test2, pred2 = None, -float(\"inf\"), -float(\"inf\"), -float(\"inf\"), float(\"inf\")\n",
    "for lam_lap in np.logspace(-5, 0, 25):\n",
    "    modeli, traini, vali, testi, predi = evaluate(zero_features=True, verbose=False, missing=False,\n",
    "                                           lam_frob=0, n_iter=1, W_lap_states=lam_lap * W_lap_states)\n",
    "    if vali > val2:\n",
    "        model2, train2, val2, test2, pred2 = modeli, traini, vali, testi, predi\n",
    "\n",
    "model3, train3, val3, test3, pred3 = None, -float(\"inf\"), -float(\"inf\"), -float(\"inf\"), float(\"inf\")\n",
    "for lam_frob in np.logspace(-5, 0, 25):\n",
    "    modeli, traini, vali, testi, predi = evaluate(zero_features=False, verbose=False, missing=False,\n",
    "                                           lam_frob=lam_frob, n_iter=1)\n",
    "    if vali > val3:\n",
    "        model3, train3, val3, test3, pred3 = modeli, traini, vali, testi, predi\n",
    "\n",
    "model4, train4, val4, test4, pred4 = None, -float(\"inf\"), -float(\"inf\"), -float(\"inf\"), float(\"inf\")\n",
    "best_lam = None\n",
    "for lam_frob in np.logspace(-5, 0, 10): \n",
    "    for lam_lap in np.logspace(-5, 0, 10):\n",
    "        modeli, traini, vali, testi, predi = evaluate(zero_features=False, verbose=False, missing=False,\n",
    "                                                   lam_frob=lam_frob, n_iter=1, W_lap_states=lam_lap * W_lap_states)\n",
    "        if vali > val4:\n",
    "            model4, train4, val4, test4, pred4 = modeli, traini, vali, testi, predi\n",
    "            best_lam= (lam_frob, lam_lap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Empirical & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train1, val1, test1, pred1))\n",
    "print(\"Constant Lap-reg & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train2, val2, test2, pred2))\n",
    "print(\"Feature-dependent & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train3, val3, test3, pred3))\n",
    "print(\"Feature-dependent Lap-reg & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train4, val4, test4, pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_lap_states = nx.adjacency_matrix(nx.path_graph(5))\n",
    "\n",
    "model5, train5, val5, test5, pred5 = evaluate(zero_features=True, verbose=False, missing=True, lam_frob=0, n_iter=1)\n",
    "model6, train6, val6, test6, pred6 = evaluate(zero_features=True, verbose=False, missing=True, lam_frob=0, n_iter=20)\n",
    "model7, train7, val7, test7, pred7 = None, -float(\"inf\"), -float(\"inf\"), -float(\"inf\"), float(\"inf\")\n",
    "for lam_lap in np.logspace(-5, -1, 10):\n",
    "    modeli, traini, vali, testi, predi = evaluate(zero_features=True, verbose=False, missing=True,\n",
    "                                           lam_frob=0, n_iter=20, W_lap_states=lam_lap * W_lap_states)\n",
    "    if vali > val7:\n",
    "        model7, train7, val7, test7, pred7 = modeli, traini, vali, testi, predi\n",
    "\n",
    "model8, train8, val8, test8, pred8 = None, -float(\"inf\"), -float(\"inf\"), -float(\"inf\"), float(\"inf\")\n",
    "for lam_frob in np.logspace(-5, 0, 10):\n",
    "    modeli, traini, vali, testi, predi = evaluate(zero_features=False, verbose=False, missing=True,\n",
    "                                           lam_frob=lam_frob, n_iter=1)\n",
    "    if vali > val8:\n",
    "        model8, train8, val8, test8, pred8 = modeli, traini, vali, testi, predi\n",
    "    \n",
    "model9, train9, val9, test9, pred9 = None, -float(\"inf\"), -float(\"inf\"), -float(\"inf\"), float(\"inf\")\n",
    "best_lam = None\n",
    "for lam_frob in np.logspace(-5, 0, 10):\n",
    "    modeli, traini, vali, testi, predi = evaluate(zero_features=False, verbose=False, missing=True,\n",
    "                                           lam_frob=lam_frob, n_iter=20)\n",
    "    if vali > val9:\n",
    "        model9, train9, val9, test9, pred9 = modeli, traini, vali, testi, predi\n",
    "        best_lam = lam_frob\n",
    "\n",
    "model10, train10, val10, test10, pred10 = None, -float(\"inf\"), -float(\"inf\"), -float(\"inf\"), float(\"inf\")\n",
    "for lam_lap in np.logspace(-5, -1, 10):\n",
    "    modeli, traini, vali, testi, predi = evaluate(zero_features=False, verbose=False, missing=True,\n",
    "                                               lam_frob=best_lam, n_iter=20, W_lap_states=lam_lap * W_lap_states)\n",
    "    if vali > val10:\n",
    "        model10, train10, val10, test10, pred10 = modeli, traini, vali, testi, predi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Constant & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train5, val5, test5, pred5))\n",
    "print(\"Constant EM & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train6, val6, test6, pred6))\n",
    "print(\"Constant EM Lap-Reg & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train7, val7, test7, pred7))\n",
    "print(\"Feature-dependent & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train8, val8, test8, pred8))\n",
    "print(\"Feature-dependent EM & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train9, val9, test9, pred9))\n",
    "print(\"Feature-dependent EM Lap-reg & %.2f & %.2f & %.2f & %.4f \\\\\\\\\" % (train10, val10, test10, pred10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fake = np.zeros((100, features.shape[1]))\n",
    "features_fake[:, 0] = np.linspace(-1, 1, 100)\n",
    "Ps1 = model10.predict(features_fake)\n",
    "features_fake = np.ones((100, features.shape[1])) * -1\n",
    "features_fake[:, 0] = np.linspace(-1, 1, 100)\n",
    "Ps2 = model10.predict(features_fake)\n",
    "features_fake = np.ones((100, features.shape[1])) * 1\n",
    "features_fake[:, 0] = np.linspace(-1, 1, 100)\n",
    "Ps3 = model10.predict(features_fake)\n",
    "\n",
    "fig, axes = plt.subplots(n, n, figsize=(20, 10))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        axes[i, j].plot(features_fake[:, 0], Ps1[:,i,j], c='k', label='RET60D=0')\n",
    "        axes[i, j].set_ylim(0, 1)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        axes[i, j].plot(features_fake[:, 0], Ps2[:,i,j], c='blue', label='RET60D=-1')\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        axes[i, j].plot(features_fake[:, 0], Ps3[:,i,j], c='red', label='RET60D=1')\n",
    "        axes[i, j].legend()\n",
    "\n",
    "for i, c in enumerate([\"low\", \"below average\", \"average\", \"above average\", \"high\"]):\n",
    "    axes[i, 0].set_ylabel(c)\n",
    "    axes[-1, i].set_xlabel(c)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/varying_vix.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fake = np.zeros((100, features.shape[1]))\n",
    "features_fake[:, 1] = np.linspace(-1, 1, 100)\n",
    "Ps1 = model10.predict(features_fake)\n",
    "features_fake = np.ones((100, features.shape[1])) * -1\n",
    "features_fake[:, 1] = np.linspace(-1, 1, 100)\n",
    "Ps2 = model10.predict(features_fake)\n",
    "features_fake = np.ones((100, features.shape[1])) * 1\n",
    "features_fake[:, 1] = np.linspace(-1, 1, 100)\n",
    "Ps3 = model10.predict(features_fake)\n",
    "\n",
    "fig, axes = plt.subplots(n, n, figsize=(20, 10))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        axes[i, j].plot(features_fake[:, 1], Ps1[:,i,j], c='k', label='VIX=0')\n",
    "        axes[i, j].set_ylim(0, 1)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        axes[i, j].plot(features_fake[:, 1], Ps2[:,i,j], c='blue', label='VIX=-1')\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        axes[i, j].plot(features_fake[:, 1], Ps3[:,i,j], c='red', label='VIX=1')\n",
    "        axes[i, j].legend()\n",
    "\n",
    "for i, c in enumerate([\"low\", \"below average\", \"average\", \"above average\", \"high\"]):\n",
    "    axes[i, 0].set_ylabel(c)\n",
    "    axes[-1, i].set_xlabel(c)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/varying_ret60d.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 20\n",
    "features_fake = np.zeros((num * num, features.shape[1]))\n",
    "x1 = np.linspace(-1, 1, num)\n",
    "x2 = np.linspace(-1, 1, num)\n",
    "for i in range(num):\n",
    "    for j in range(num):\n",
    "        features_fake[i + j * num, 0] = x1[i]\n",
    "        features_fake[i + j * num, 1] = x2[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ps = model10.predict(features_fake)\n",
    "plt.imshow(Ps[:, 4, 4].reshape(num, num), vmin=0, vmax=1)\n",
    "plt.xlabel(\"VIX\")\n",
    "plt.ylabel(\"RET60D\")\n",
    "plt.xticks(np.linspace(0, num-1, num //2), np.round(np.linspace(-1,1,num // 2), 1))\n",
    "plt.yticks(np.linspace(0, num-1, num //2), np.round(np.linspace(-1,1,num // 2), 1))\n",
    "plt.title(\"$P_{55}$\")\n",
    "plt.colorbar()\n",
    "plt.savefig(\"figs/P_55.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ps = model10.predict(features_fake)\n",
    "plt.imshow(Ps[:, 2, 3].reshape(num, num), vmin=0, vmax=1)\n",
    "plt.xlabel(\"VIX\")\n",
    "plt.ylabel(\"RET60D\")\n",
    "plt.xticks(np.linspace(0, num-1, num //2), np.round(np.linspace(-1,1,num // 2), 1))\n",
    "plt.yticks(np.linspace(0, num-1, num //2), np.round(np.linspace(-1,1,num // 2), 1))\n",
    "plt.title(\"$P_{34}$\")\n",
    "plt.colorbar()\n",
    "plt.savefig(\"figs/P_34.pdf\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
